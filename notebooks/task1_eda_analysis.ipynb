{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Exploratory Data Analysis (EDA)\n",
        "\n",
        "This notebook performs comprehensive EDA on the Financial News and Stock Price Integration Dataset (FNSPID).\n",
        "\n",
        "## Analysis Components:\n",
        "1. **Descriptive Statistics**: Textual lengths, articles per publisher, publication dates\n",
        "2. **Text Analysis**: Keywords, phrases, topic modeling\n",
        "3. **Time Series Analysis**: Publication frequency over time, publishing times\n",
        "4. **Publisher Analysis**: Publisher distribution, content analysis, domain analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('../scripts')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Import custom modules\n",
        "from load_data import load_financial_news_data, validate_data\n",
        "from eda_descriptive_stats import (\n",
        "    calculate_text_statistics,\n",
        "    count_articles_per_publisher,\n",
        "    analyze_publication_dates,\n",
        "    plot_text_statistics,\n",
        "    plot_publisher_analysis,\n",
        "    plot_publication_trends\n",
        ")\n",
        "from eda_text_analysis import (\n",
        "    extract_keywords,\n",
        "    extract_phrases,\n",
        "    identify_financial_keywords,\n",
        "    perform_topic_modeling,\n",
        "    display_topics,\n",
        "    create_wordcloud,\n",
        "    plot_keywords\n",
        ")\n",
        "from eda_time_series import (\n",
        "    analyze_publication_frequency,\n",
        "    analyze_publishing_times,\n",
        "    identify_market_events,\n",
        "    plot_publication_frequency,\n",
        "    plot_publishing_times,\n",
        "    plot_spikes\n",
        ")\n",
        "from eda_publisher_analysis import (\n",
        "    analyze_publishers,\n",
        "    identify_publisher_domains,\n",
        "    analyze_publisher_content,\n",
        "    analyze_publisher_timing,\n",
        "    plot_publisher_distribution,\n",
        "    plot_publisher_content_analysis,\n",
        "    plot_domain_analysis\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data_path = '../data/raw_analyst_ratings.csv'\n",
        "\n",
        "print(\"Loading financial news data...\")\n",
        "df = load_financial_news_data(data_path)\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "display(df.head())\n",
        "\n",
        "# Note: This is a large dataset (1.4M+ rows)\n",
        "# For faster initial exploration, you can sample the data:\n",
        "# df_sample = df.sample(n=100000, random_state=42)  # Sample 100k rows\n",
        "# Or filter by date range:\n",
        "# df_sample = df[df['date'] >= '2019-01-01']  # Recent data only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate data\n",
        "validation_report = validate_data(df)\n",
        "print(\"Data Validation Report:\")\n",
        "print(\"=\" * 50)\n",
        "for key, value in validation_report.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Descriptive Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate text statistics\n",
        "text_stats, df_with_stats = calculate_text_statistics(df)\n",
        "print(\"Text Statistics:\")\n",
        "print(\"=\" * 50)\n",
        "display(text_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot text statistics\n",
        "plot_text_statistics(df_with_stats, save_path='../outputs/text_statistics.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count articles per publisher\n",
        "publisher_counts = count_articles_per_publisher(df)\n",
        "print(\"Top 20 Publishers:\")\n",
        "print(\"=\" * 50)\n",
        "display(publisher_counts.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot publisher analysis\n",
        "plot_publisher_analysis(publisher_counts, top_n=20, save_path='../outputs/publisher_distribution.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze publication dates\n",
        "date_analysis, df_with_dates = analyze_publication_dates(df)\n",
        "print(\"Publication Date Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total days covered: {date_analysis['total_days']}\")\n",
        "print(f\"Date range: {date_analysis['date_range']['start']} to {date_analysis['date_range']['end']}\")\n",
        "print(\"\\nArticles per weekday:\")\n",
        "for day, count in date_analysis['articles_per_weekday'].items():\n",
        "    print(f\"  {day}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot publication trends\n",
        "plot_publication_trends(date_analysis, save_path='../outputs/publication_trends.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Analysis and Topic Modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract keywords\n",
        "keywords_df = extract_keywords(df, top_n=50)\n",
        "print(\"Top 50 Keywords:\")\n",
        "print(\"=\" * 50)\n",
        "display(keywords_df.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot keywords\n",
        "plot_keywords(keywords_df, top_n=30, save_path='../outputs/top_keywords.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract phrases (bigrams)\n",
        "bigrams_df = extract_phrases(df, n_grams=2, top_n=30)\n",
        "print(\"Top 30 Bigrams:\")\n",
        "print(\"=\" * 50)\n",
        "display(bigrams_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify financial keywords\n",
        "financial_keywords = identify_financial_keywords(df)\n",
        "print(\"Financial Keywords Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "for category, data in financial_keywords.items():\n",
        "    print(f\"\\n{category.upper()}:\")\n",
        "    print(f\"  Count: {data['count']}\")\n",
        "    print(f\"  Percentage: {data['percentage']:.2f}%\")\n",
        "    if data['sample_headlines']:\n",
        "        print(f\"  Sample headlines:\")\n",
        "        for headline in data['sample_headlines'][:3]:\n",
        "            print(f\"    - {headline[:80]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create word cloud\n",
        "try:\n",
        "    create_wordcloud(df, save_path='../outputs/wordcloud.png')\n",
        "except Exception as e:\n",
        "    print(f\"Word cloud creation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Topic Modeling (if gensim is available)\n",
        "try:\n",
        "    print(\"Performing topic modeling...\")\n",
        "    lda_model, doc_topics = perform_topic_modeling(df, num_topics=10, passes=10)\n",
        "    \n",
        "    # Display topics\n",
        "    topics_df = display_topics(lda_model, num_words=10)\n",
        "    print(\"\\nIdentified Topics:\")\n",
        "    print(\"=\" * 50)\n",
        "    display(topics_df)\n",
        "except Exception as e:\n",
        "    print(f\"Topic modeling failed: {e}\")\n",
        "    print(\"Make sure gensim is installed: pip install gensim\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Time Series Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze publication frequency\n",
        "frequency_analysis = analyze_publication_frequency(df)\n",
        "print(\"Publication Frequency Statistics:\")\n",
        "print(\"=\" * 50)\n",
        "stats = frequency_analysis['statistics']\n",
        "for key, value in stats.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot publication frequency\n",
        "plot_publication_frequency(frequency_analysis, save_path='../outputs/publication_frequency.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze publishing times\n",
        "timing_analysis = analyze_publishing_times(df)\n",
        "print(\"Publishing Time Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Most active hour: {timing_analysis['most_active_hour']}:00\")\n",
        "print(f\"Least active hour: {timing_analysis['least_active_hour']}:00\")\n",
        "print(\"\\nPeak hours:\")\n",
        "display(timing_analysis['peak_hours'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot publishing times\n",
        "plot_publishing_times(timing_analysis, save_path='../outputs/publishing_times.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify market events (publication spikes)\n",
        "spikes = identify_market_events(df, spike_threshold=2.0)\n",
        "print(f\"Identified {len(spikes)} potential market events (publication spikes):\")\n",
        "print(\"=\" * 50)\n",
        "if len(spikes) > 0:\n",
        "    display(spikes.head(10))\n",
        "    plot_spikes(spikes.head(20), save_path='../outputs/market_events.png')\n",
        "else:\n",
        "    print(\"No significant spikes identified.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Publisher Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze publishers\n",
        "publisher_analysis = analyze_publishers(df)\n",
        "print(\"Publisher Analysis Statistics:\")\n",
        "print(\"=\" * 50)\n",
        "stats = publisher_analysis['statistics']\n",
        "for key, value in stats.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot publisher distribution\n",
        "plot_publisher_distribution(publisher_analysis, top_n=20, save_path='../outputs/publisher_analysis.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify publisher domains\n",
        "domain_counts = identify_publisher_domains(df)\n",
        "print(\"Publisher Domain Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "display(domain_counts.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot domain analysis\n",
        "plot_domain_analysis(domain_counts, top_n=15, save_path='../outputs/domain_analysis.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze publisher content\n",
        "content_analysis = analyze_publisher_content(df, top_n=10)\n",
        "print(\"Publisher Content Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "content_df = pd.DataFrame(content_analysis).T\n",
        "display(content_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot publisher content analysis\n",
        "plot_publisher_content_analysis(content_analysis, save_path='../outputs/publisher_content.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze publisher timing\n",
        "timing_analysis_by_publisher = analyze_publisher_timing(df, top_n=10)\n",
        "print(\"Publisher Timing Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "timing_df = pd.DataFrame(timing_analysis_by_publisher).T\n",
        "display(timing_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary and Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"EDA Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nDataset Overview:\")\n",
        "print(f\"  Total articles: {len(df)}\")\n",
        "print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "print(f\"  Unique publishers: {df['publisher'].nunique()}\")\n",
        "print(f\"  Unique stocks: {df['stock'].nunique() if 'stock' in df.columns else 'N/A'}\")\n",
        "\n",
        "print(f\"\\nKey Insights:\")\n",
        "print(f\"  1. Average headline length: {df_with_stats['headline_length'].mean():.1f} characters\")\n",
        "print(f\"  2. Top publisher: {publisher_analysis['statistics']['top_publisher']} ({publisher_analysis['statistics']['top_publisher_percentage']:.1f}% of articles)\")\n",
        "print(f\"  3. Most active publishing hour: {timing_analysis['most_active_hour']}:00\")\n",
        "print(f\"  4. Market concentration (HHI): {publisher_analysis['statistics']['hhi']:.0f} ({publisher_analysis['statistics']['concentration_level']})\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
